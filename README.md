# llama-cpp-fastapi-service
Use Fastapi to serve the llama 2 cpp fastapi service.
